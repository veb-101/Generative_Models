{"cells":[{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"!pip install -Uq pip\n!pip install -q torchsummary","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"import os\n# import re\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nimport torchvision.transforms as T\nfrom torchvision.utils import make_grid\n\nfrom torchsummary import summary","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Constants"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"latent_dims = 128\nnum_epochs = 150\nvariational_beta = 1\nbatch_size = 256\ncapacity = 64\nlearning_rate = 1e-4\nimage_channels = 3","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utility Functions"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else: \n        return torch.device('cpu')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"device = get_default_device()\ndevice","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\n\nclass DeviceDataLoader():\n\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n\n    def __iter__(self):\n        for b in self.dl:\n            yield to_device(b, self.device)\n    \n    def __len__(self):\n        return len(self.dl)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import cv2\n\ndef visualize_reconstruction(model, batch_images, iter_num, save_dir=\"./images\"):\n    \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    with torch.no_grad():\n        recons_images, _, _ = model(batch_images)\n        img_ = make_grid(recons_images[:50].cpu().detach().clamp(0.0, 1.0) * 255.0, nrow=10).permute((1, 2, 0)).numpy()\n        cv2.imwrite(os.path.join(save_dir, f\"Image_{iter_num:04}.png\"), img_)\n\ndef generate_images(model, latent_vectors, iter_num, save_dir=\"./generated\"):\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    with torch.no_grad():\n        # model.to(\"cpu\")\n        img_generated = model.decoder(latent_vectors)\n\n        img_ = make_grid(img_generated[:100].cpu().detach().clamp(0.0, 1.0) * 255.0, nrow=10).permute((1, 2, 0)).numpy()\n        cv2.imwrite(os.path.join(save_dir, f\"Image_{iter_num:04}.png\"), img_)\n        # model.to(device)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset preparation"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"dataset_path = r\"../input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled\"\n# print(os.listdir(dataset_path))","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Seperate Train and Validation images"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_image_paths(IMG_DIR):\n    \n    total_files = 0\n    all_filenames = []\n    \n    for root, _, files in itertools.islice(os.walk(IMG_DIR), 0, None):\n        if len(files) == 0:\n            continue\n        \n        for file_name in files:\n            total_files += 1\n            file_path = os.path.join(root, file_name)\n            \n            all_filenames.append(file_path)\n        \n    \n    print(f\"Total Faces: {total_files}\")\n    train_size = int(total_files * 0.9)\n    val_size = int(total_files * 0.1) + 1\n    \n#     print(train_size, val_size, train_size + val_size)\n    \n    train_set, val_set = random_split(all_filenames, [train_size, val_size])\n    print(f\"Training Set: {len(train_set)}\")\n    print(f\"Validation Set: {len(val_set)}\")\n    return train_set, val_set","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_paths, validation_paths = get_image_paths(dataset_path)","execution_count":10,"outputs":[{"output_type":"stream","text":"Total Faces: 13233\nTraining Set: 11909\nValidation Set: 1324\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Prepare dataset"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class LBW_Dataset(Dataset):\n    '''\n        Parse raw data to form a Dataset of (X, y).\n    '''\n    def __init__(self, image_paths, transform=None):\n        self.image_paths = image_paths\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx])\n        img = img.resize((64, 64), Image.BICUBIC)\n        img = np.asarray(img, dtype='float32')\n        img = img / 255.0\n        img = self.transform(img)\n        return img","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataLoaders"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"transformation = T.ToTensor()\n\ntrain_ds = LBW_Dataset(train_paths, transformation)\nvalidation_ds = LBW_Dataset(validation_paths, transformation)\n\nlen(train_ds), len(validation_ds)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(11909, 1324)"},"metadata":{}}]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, \n                  num_workers=3, pin_memory=True)\n\nvalid_dl = DataLoader(validation_ds, batch_size*2, \n                num_workers=2, pin_memory=True)\n\n\n# Move to device\ntrain_loader = DeviceDataLoader(train_dl, device)\nval_loader = DeviceDataLoader(valid_dl, device)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convolutional Variational Auto-encoder "},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self):\n        super().__init__() \n        c = capacity\n        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=c//2, kernel_size=4, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=c//2, out_channels=c, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=c, out_channels=c, kernel_size=4, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1)\n        self.conv5 = nn.Conv2d(in_channels=c*2, out_channels=c*2, kernel_size=4, stride=2, padding=1)\n    \n    \n        self.fc_1 = nn.Linear(in_features=c*2*2*2, out_features=512)\n        self.fc_2 = nn.Linear(in_features=512, out_features=1024)\n        self.fc_3 = nn.Linear(in_features=1024, out_features=512)\n    \n        self.softplus_operation = nn.Softplus()\n        \n        self.fc_mu = nn.Linear(in_features=512, out_features=latent_dims)\n        self.fc_logvar = nn.Linear(in_features=512, out_features=latent_dims)\n\n    def forward(self, xb):\n        xb = F.relu(self.conv1(xb))\n        xb = F.relu(self.conv2(xb))\n        xb = F.relu(self.conv3(xb))\n        xb = F.relu(self.conv4(xb))\n        xb = F.relu(self.conv5(xb))\n\n        xb = xb.view(xb.size(0), -1)\n\n        xb_out = F.relu(self.fc_1(xb))\n        xb = F.relu(self.fc_2(xb_out))\n        \n        xb = F.relu(self.fc_3(xb) + xb_out)\n        \n        x_mu = self.fc_mu(xb)\n        x_logvar = self.softplus_operation(self.fc_logvar(xb))\n        \n        return x_mu, x_logvar\n        \n\nclass Decoder(nn.Module):\n\n    \n    def __init__(self):\n        super().__init__() \n        c = capacity\n        self.fc4 = nn.Linear(latent_dims, 512)\n        self.fc3 = nn.Linear(512, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc1 = nn.Linear(512, 512)\n\n#         self.upsampler = nn.UpsamplingBilinear2d(scale_factor=2)\n        self.upsampler = nn.Upsample(scale_factor=2, mode='nearest')\n        \n        self.conv5 = nn.Conv2d(in_channels=c*2, out_channels=c*2, kernel_size=3, stride=1,padding=1)\n        self.conv4 = nn.Conv2d(in_channels=c*2, out_channels=c,kernel_size=3,stride=1,padding=1)\n        self.conv3 = nn.Conv2d(in_channels=c, out_channels=c, kernel_size=3,stride=1,padding=1)\n        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c//2, kernel_size=3, stride=1,padding=1)\n        self.conv1 = nn.Conv2d(in_channels=c//2, out_channels=image_channels, kernel_size=3, stride=1,padding=1)\n\n\n    def forward(self, xb):\n        xb_out = F.relu(self.fc4(xb))\n        xb = F.relu(self.fc3(xb_out))\n        xb = F.relu(self.fc2(xb) + xb_out)\n        xb = F.relu(self.fc1(xb))\n        xb = xb.view(xb.size(0), capacity*2, 2, 2)\n        \n        xb = self.upsampler(xb)\n        xb = F.relu(self.conv5(xb))\n        \n        xb = self.upsampler(xb)\n        xb = F.relu(self.conv4(xb))\n        \n        xb = self.upsampler(xb)\n        xb = F.relu(self.conv3(xb))\n        \n        xb = self.upsampler(xb)\n        xb = F.relu(self.conv2(xb))\n        \n        xb = self.upsampler(xb)\n        xb = torch.sigmoid(self.conv1(xb))\n            \n        return xb\n          \n#         self.conv3 = nn.ConvTranspose2d(in_channels=c, out_channels=c, kernel_size=4, stride=2, padding=1, output_padding=1)\n#         self.conv2 = nn.ConvTranspose2d(in_channels=c, out_channels=c, kernel_size=4, stride=2, padding=1)\n#         self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)\n        # self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=c//4, kernel_size=4, stride=2, padding=1)\n\n        # self.fc_penultimate = nn.Linear(in_features=c//4*28*28, out_features=28*28)\n        # self.fc_final = nn.Linear(in_features=28*28, out_features=28*28)\n\n#     def forward(self, xb):\n#         xb = F.relu(self.fc_2(xb))\n#         xb = F.relu(self.fc_1(xb))\n\n#         xb = xb.view(xb.size(0), capacity, 3, 3)\n\n#         xb = F.relu(self.conv3(xb))\n#         xb = F.relu(self.conv2(xb))\n#         xb = torch.sigmoid(self.conv1(xb))\n\n        # xb = xb.view(xb.size(0), -1)\n\n        # xb = self.fc_penultimate(xb)\n        # xb = torch.sigmoid(self.fc_final(xb))\n        # xb = xb.view(xb.size(0), 1, 28, 28)\n#         return xb\n\ndef vae_loss(reconstruct, og, mu, logvar):\n    reconstruction_loss = F.binary_cross_entropy(reconstruct.view(-1, 12288), og.view(-1, 12288), reduction='sum')\n    # kl_divergence = 0.5 * torch.sum(torch.exp(logvar) + torch.square(mu) - 1 - logvar)\n    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return reconstruction_loss + variational_beta * kl_divergence\n    \n\nclass VariationalAutoEncoder(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        \n    def latent_sample(self, mu, logvar):\n        sigma = torch.mul(logvar, 0.5)\n        eps = torch.randn_like(sigma)\n        sample = mu + (sigma * eps)\n        return sample\n    \n    def forward(self, xb):\n        latent_mu, latent_std = self.encoder(xb)\n        latent_ = self.latent_sample(latent_mu, latent_std)\n        reconstruction = self.decoder(latent_)\n        return reconstruction, latent_mu, latent_std\n    \n    def train_step(self, input_batch):\n        input_batch_reconstruct, batch_mu, batch_std = self(input_batch)\n        loss = vae_loss(input_batch_reconstruct, input_batch, batch_mu, batch_std)\n        return {\"loss\": loss}\n    \n    def valid_step(self,val_batch):\n        with torch.no_grad():\n            val_batch_reconstruct, batch_mu, batch_std = self(val_batch)\n            loss = vae_loss(val_batch_reconstruct, val_batch, batch_mu, batch_std)\n            \n        return {\"val_loss\": loss}\n\n    def get_metrics_epoch_end(self, outputs, validation=True):\n        if validation:\n            loss_ = 'val_loss'\n        else:\n            loss_ = 'loss'\n            \n        batch_losses = [x[f'{loss_}'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   \n        return {f'{loss_}': epoch_loss.item()}\n\n    def epoch_end(self, epoch, result):\n        print(f\"Epoch [{epoch+1}] -> last_lr: {result['lrs'][-1]:.4f}, loss: {result['loss']:.4f}, val_loss: {result['val_loss']:.4f}\")","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# summary(to_device(Encoder(), device), (3, 64, 64))\n# summary(to_device(Decoder(), device), (1, latent_dims))\nsummary(to_device(VariationalAutoEncoder(), device), (3, 64, 64))","execution_count":15,"outputs":[{"output_type":"stream","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 32, 32]           1,568\n            Conv2d-2           [-1, 64, 16, 16]          32,832\n            Conv2d-3             [-1, 64, 8, 8]          65,600\n            Conv2d-4            [-1, 128, 4, 4]         131,200\n            Conv2d-5            [-1, 128, 2, 2]         262,272\n            Linear-6                  [-1, 512]         262,656\n            Linear-7                 [-1, 1024]         525,312\n            Linear-8                  [-1, 512]         524,800\n            Linear-9                  [-1, 128]          65,664\n           Linear-10                  [-1, 128]          65,664\n         Softplus-11                  [-1, 128]               0\n          Encoder-12     [[-1, 128], [-1, 128]]               0\n           Linear-13                  [-1, 512]          66,048\n           Linear-14                 [-1, 1024]         525,312\n           Linear-15                  [-1, 512]         524,800\n           Linear-16                  [-1, 512]         262,656\n         Upsample-17            [-1, 128, 4, 4]               0\n           Conv2d-18            [-1, 128, 4, 4]         147,584\n         Upsample-19            [-1, 128, 8, 8]               0\n           Conv2d-20             [-1, 64, 8, 8]          73,792\n         Upsample-21           [-1, 64, 16, 16]               0\n           Conv2d-22           [-1, 64, 16, 16]          36,928\n         Upsample-23           [-1, 64, 32, 32]               0\n           Conv2d-24           [-1, 32, 32, 32]          18,464\n         Upsample-25           [-1, 32, 64, 64]               0\n           Conv2d-26            [-1, 3, 64, 64]             867\n          Decoder-27            [-1, 3, 64, 64]               0\n================================================================\nTotal params: 3,594,019\nTrainable params: 3,594,019\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 2.65\nParams size (MB): 13.71\nEstimated Total Size (MB): 16.41\n----------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # pip install -qU torchviz\n# from torchviz import make_dot\n# x = torch.zeros(1, 3, 64, 64, dtype=torch.float, requires_grad=False)\n# model = Encoder()\n# outs = model(x)\n# make_dot(outs[0])\n\n# input_names = [\"Input\"]\n# output_names = [\"Mu\", \"LogVar\"]\n# torch.onnx.export(model, x, 'Encoder.onnx', input_names=input_names, output_names=output_names)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf ./images\n!rm -rf ./generated","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for visualizing reconstruction\nfor val_batch in val_loader:\n    val_images = val_batch\n    break\n\n\n# for generating new images\n\n# latents = torch.nn.init.normal_(torch.FloatTensor(50, latent_dims), mean=0.0, std=1.0)\n# latents = to_device(torch.FloatTensor(100, latent_dims).normal_(0.0, 2.0), device)\nlatents = torch.randn(128, latent_dims, device=device)\n\nlatents = to_device(latents, device)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.valid_step(val_batch) for val_batch in val_loader]\n    return model.get_metrics_epoch_end(outputs, validation=True)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr(optimizer: object) -> float:\n    ''' Returns current learning rate'''\n\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(epochs, lr, model, train_loader, val_loader, opt_func=None):\n    history = []\n    \n    if not opt_func:\n        optimizer = torch.optim.SGD(model.parameters(), lr)\n    else:\n        optimizer = opt_func\n\n    \n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=0.1,\n                                                    epochs=epochs, \n                                                    steps_per_epoch=len(train_loader))\n    \n\n    for epoch in range(epochs):\n        # Training Phase \n        train_history = []\n        lrs = []\n        \n        # new image generation\n        generate_images(model, latents, epoch)\n\n        model.train()\n        \n        for train_batch  in train_loader:\n            info = model.train_step(train_batch)\n            loss = info['loss']\n        \n            # contains batch loss for training phase\n            train_history.append(info)\n            \n            loss.backward()\n\n            nn.utils.clip_grad_value_(model.parameters(), 0.1)\n\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            lrs.append(get_lr(optimizer))\n            \n            scheduler.step()\n            \n\n        train_result = model.get_metrics_epoch_end(train_history, validation=False)\n        val_result = evaluate(model, val_loader)\n        result = {**train_result, **val_result}\n        result['lrs'] = lrs\n        visualize_reconstruction(model, val_images, epoch+1)\n\n        model.epoch_end(epoch, result)\n        history.append(result)\n\n    return history","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = VariationalAutoEncoder()\nto_device(model, device)\nsummary(model, (3, 64, 64))","execution_count":22,"outputs":[{"output_type":"stream","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 32, 32]           1,568\n            Conv2d-2           [-1, 64, 16, 16]          32,832\n            Conv2d-3             [-1, 64, 8, 8]          65,600\n            Conv2d-4            [-1, 128, 4, 4]         131,200\n            Conv2d-5            [-1, 128, 2, 2]         262,272\n            Linear-6                  [-1, 512]         262,656\n            Linear-7                 [-1, 1024]         525,312\n            Linear-8                  [-1, 512]         524,800\n            Linear-9                  [-1, 128]          65,664\n           Linear-10                  [-1, 128]          65,664\n         Softplus-11                  [-1, 128]               0\n          Encoder-12     [[-1, 128], [-1, 128]]               0\n           Linear-13                  [-1, 512]          66,048\n           Linear-14                 [-1, 1024]         525,312\n           Linear-15                  [-1, 512]         524,800\n           Linear-16                  [-1, 512]         262,656\n         Upsample-17            [-1, 128, 4, 4]               0\n           Conv2d-18            [-1, 128, 4, 4]         147,584\n         Upsample-19            [-1, 128, 8, 8]               0\n           Conv2d-20             [-1, 64, 8, 8]          73,792\n         Upsample-21           [-1, 64, 16, 16]               0\n           Conv2d-22           [-1, 64, 16, 16]          36,928\n         Upsample-23           [-1, 64, 32, 32]               0\n           Conv2d-24           [-1, 32, 32, 32]          18,464\n         Upsample-25           [-1, 32, 64, 64]               0\n           Conv2d-26            [-1, 3, 64, 64]             867\n          Decoder-27            [-1, 3, 64, 64]               0\n================================================================\nTotal params: 3,594,019\nTrainable params: 3,594,019\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 2.65\nParams size (MB): 13.71\nEstimated Total Size (MB): 16.41\n----------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = [evaluate(model, val_loader)]\nhistory","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"[{'val_loss': 3792803.0}]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=1e-4)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = fit(150, learning_rate, model, train_loader, val_loader, optimizer)","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch [1] -> last_lr: 0.0041, loss: 2110909.5000, val_loss: 3457486.0000\nEpoch [2] -> last_lr: 0.0045, loss: 1983661.2500, val_loss: 3480033.0000\nEpoch [3] -> last_lr: 0.0050, loss: 1980685.0000, val_loss: 3379997.0000\nEpoch [4] -> last_lr: 0.0058, loss: 1995939.6250, val_loss: 3559743.5000\nEpoch [5] -> last_lr: 0.0069, loss: 2001709.8750, val_loss: 3383755.0000\nEpoch [6] -> last_lr: 0.0081, loss: 1960210.3750, val_loss: 3472971.0000\nEpoch [7] -> last_lr: 0.0096, loss: 1970040.5000, val_loss: 3361466.5000\nEpoch [8] -> last_lr: 0.0113, loss: 2031578.3750, val_loss: 3544373.5000\nEpoch [9] -> last_lr: 0.0131, loss: 2044796.2500, val_loss: 3506155.0000\nEpoch [10] -> last_lr: 0.0152, loss: 2006746.5000, val_loss: 3479946.7500\nEpoch [11] -> last_lr: 0.0174, loss: 2034279.8750, val_loss: 3469672.7500\nEpoch [12] -> last_lr: 0.0198, loss: 2007291.8750, val_loss: 3508217.5000\nEpoch [13] -> last_lr: 0.0224, loss: 2023592.1250, val_loss: 3483720.5000\nEpoch [14] -> last_lr: 0.0251, loss: 2004951.7500, val_loss: 3524578.7500\nEpoch [15] -> last_lr: 0.0280, loss: 2026781.3750, val_loss: 3500912.0000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model, f\"./vae_{latent_dims}.pt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import imageio\nimport numpy as np\nimport glob\nfrom PIL import Image \nfrom numpy import asarray\nimport IPython.display as dsip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reconstruct_file = './reconstruction.gif'\n\n\nfilenames = glob.glob('./images/*.png')\nfilenames = sorted(filenames)\nimgs = [asarray(Image.open(img)) for img in filenames]\nimageio.mimsave(reconstruct_file, imgs)\n\nwith open(reconstruct_file,'rb') as file:\n    dsip.display(dsip.Image(file.read()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generated_file = './generation.gif'\n\n\nfilenames = glob.glob('./generated/*.png')\nfilenames = sorted(filenames)\nimgs = [asarray(Image.open(img)) for img in filenames]\nimageio.mimsave(reconstruct_file, imgs)\n\nwith open(generated_file,'rb') as file:\n    dsip.display(dsip.Image(file.read()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\nwith torch.no_grad():\n\n    # sample latent vectors from the normal distribution\n    latent = torch.randn(128, latent_dims, device=device)\n\n    # reconstruct images from the latent vectors\n    img_recon = model.decoder(latent)\n    # img_recon = img_recon.cpu()\n\n    fig, ax = plt.subplots(figsize=(5, 5))\n    # plt.imshow(make_grid(img_recon.data[:100].cpu().detach(),10,5))\n    plt.axis(\"off\")\n    plt.imshow(make_grid(img_recon.data[:100].cpu().detach().clamp(0.0, 1.0), nrow=10).permute((1, 2, 0)).numpy())\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}